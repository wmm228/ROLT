# Multi-hop Question Answering with MCTS and IRCoT

è¿™æ˜¯ä¸€ä¸ªåŸºäºè’™ç‰¹å¡æ´›æ ‘æœç´¢ï¼ˆMCTSï¼‰å’Œé“¾å¼æ€ç»´æ£€ç´¢ï¼ˆIRCoTï¼‰çš„å¤šè·³é—®ç­”ç ”ç©¶é¡¹ç›®ï¼Œä¸“æ³¨äºçŸ¥è¯†å¯†é›†å‹å¤šæ­¥éª¤é—®é¢˜çš„è‡ªåŠ¨æ¨ç†å’Œå›ç­”ã€‚

## ğŸš€ é¡¹ç›®ç‰¹è‰²

- **MCTSæœç´¢ç®—æ³•**ï¼šä½¿ç”¨è’™ç‰¹å¡æ´›æ ‘æœç´¢è¿›è¡Œæ™ºèƒ½é—®é¢˜åˆ†è§£å’Œæ¨ç†è·¯å¾„æ¢ç´¢
- **IRCoTæ¡†æ¶**ï¼šé›†æˆé“¾å¼æ€ç»´æ£€ç´¢ï¼Œäº¤æ›¿è¿›è¡Œæ£€ç´¢å’Œæ¨ç†
- **è·³æ•°é¢„æµ‹**ï¼šè‡ªåŠ¨é¢„æµ‹é—®é¢˜çš„å¤æ‚åº¦ï¼ˆ2è·³/3è·³ï¼‰ï¼ŒåŠ¨æ€è°ƒæ•´æœç´¢ç­–ç•¥
- **å¤šæ¨¡å‹æ”¯æŒ**ï¼šæ”¯æŒGPTç³»åˆ—ã€FLAN-T5ç³»åˆ—ç­‰å¤šç§è¯­è¨€æ¨¡å‹
- **BGEåµŒå…¥æ£€ç´¢**ï¼šä½¿ç”¨BGE-M3æ¨¡å‹è¿›è¡Œé«˜è´¨é‡æ–‡æ¡£åµŒå…¥å’Œæ£€ç´¢
- **æŠ•ç¥¨æœºåˆ¶**ï¼šé€šè¿‡å¤šè·¯å¾„æ¨ç†ç»“æœæŠ•ç¥¨ç¡®å®šæœ€ç»ˆç­”æ¡ˆ

## ğŸ“ é¡¹ç›®ç»“æ„

```
rolt/
â”œâ”€â”€ main.py                 # ä¸»æ‰§è¡Œå…¥å£ï¼ŒMCTSæœç´¢é€»è¾‘
â”œâ”€â”€ api.py                  # APIæ¥å£ï¼Œé—®ç­”å¤„ç†é€»è¾‘
â”œâ”€â”€ llm_client.py          # LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šç§æ¨¡å‹è°ƒç”¨
â”œâ”€â”€ mcts_all.py            # MCTSç®—æ³•å®Œæ•´å®ç°
â”œâ”€â”€ bge.py                 # BGEåµŒå…¥æ¨¡å‹å°è£…
â”œâ”€â”€ ircot-main/            # IRCoTæ¡†æ¶å®Œæ•´å®ç°
â”‚   â”œâ”€â”€ commaqa/           # æ ¸å¿ƒæ¨ç†æ¡†æ¶
â”‚   â”œâ”€â”€ retriever_server/  # æ£€ç´¢æœåŠ¡å™¨
â”‚   â””â”€â”€ llm_server/        # LLMæœåŠ¡å™¨
â”œâ”€â”€ hop_pre/               # è·³æ•°é¢„æµ‹æ¨¡å‹
â”‚   â”œâ”€â”€ train_model.py     # æ¨¡å‹è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ process_data.py    # æ•°æ®é¢„å¤„ç†
â””â”€â”€ intro/                 # æ¨¡å‹è¯„ä¼°å’Œæ¯”è¾ƒ
    â”œâ”€â”€ gpt4o.py          # GPT-4oè¯„ä¼°
    â””â”€â”€ model_score.py    # æ¨¡å‹è¯„åˆ†æ¯”è¾ƒ
```

## ğŸ› ï¸ å®‰è£…é…ç½®

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- CUDA 11.0+ (æ¨è)
- 8GB+ GPUå†…å­˜

### å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# è®¾ç½®GPUè®¾å¤‡
export CUDA_VISIBLE_DEVICES="0,1,2,3"

# è®¾ç½®OpenAI API (å¦‚æœä½¿ç”¨GPTæ¨¡å‹)
export OPENAI_API_KEY="your_api_key_here"
```

## ğŸ“Š æ•°æ®å‡†å¤‡

### æ”¯æŒçš„æ•°æ®é›†

- **HotpotQA**: å¤šè·³æ¨ç†é—®ç­”æ•°æ®é›†
- **2WikiMultihopQA**: åŸºäºç»´åŸºç™¾ç§‘çš„å¤šè·³é—®ç­”
- **MuSiQue**: å¤šæ­¥æ¨ç†é—®ç­”
- **IIRC**: ä¸å®Œæ•´ä¿¡æ¯é˜…è¯»ç†è§£

### æ•°æ®ä¸‹è½½

```bash
# ä¸‹è½½é¢„å¤„ç†æ•°æ®
cd ircot-main
./download/processed_data.sh

# ä¸‹è½½åŸå§‹æ•°æ®ï¼ˆç”¨äºæ„å»ºæ£€ç´¢ç´¢å¼•ï¼‰
./download/raw_data.sh
```

### æ•°æ®æ ¼å¼

æ”¯æŒæ ‡å‡†çš„JSONæ ¼å¼ï¼ŒåŒ…å«é—®é¢˜ã€ä¸Šä¸‹æ–‡å’Œç­”æ¡ˆï¼š

```json
{
  "question": "Who lived longer, Muhammad Ali or Alan Turing?",
  "context": [["Muhammad Ali", ["Muhammad Ali was born in 1942..."]], ...],
  "answer": "Muhammad Ali"
}
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```bash
# è¿è¡Œä¸»ç¨‹åºï¼ˆä½¿ç”¨MCTSæœç´¢ï¼‰
python main.py

# è¿è¡ŒAPIæ¥å£
python api.py
```

### 2. è·³æ•°é¢„æµ‹æ¨¡å‹è®­ç»ƒ

```bash
cd hop_pre
python train_model.py
```

### 3. å¯åŠ¨æ£€ç´¢æœåŠ¡

```bash
# å¯åŠ¨Elasticsearch (éœ€è¦å…ˆå®‰è£…)
./bin/elasticsearch

# å¯åŠ¨æ£€ç´¢æœåŠ¡å™¨
cd ircot-main
uvicorn serve:app --port 8000 --app-dir retriever_server

# æ„å»ºç´¢å¼•
python retriever_server/build_index.py hotpotqa
```

### 4. å¯åŠ¨LLMæœåŠ¡

```bash
# å¯åŠ¨FLAN-T5æ¨¡å‹æœåŠ¡
cd ircot-main
MODEL_NAME=flan-t5-xl uvicorn serve:app --port 8010 --app-dir llm_server
```

## âš™ï¸ é…ç½®è¯´æ˜

### ä¸»è¦å‚æ•°

åœ¨ `main.py` ä¸­å¯ä»¥é…ç½®çš„å…³é”®å‚æ•°ï¼š

```python
# LLMé…ç½®
base_url = "https://api.agicto.cn/v1"
api_key = "your_api_key"

# MCTSå‚æ•°
B = 4                    # æ¯æ­¥ç”Ÿæˆå­é—®é¢˜æ•°
max_hop = 3             # æœ€å¤§è·³æ•°
max_iter = 15           # æœ€å¤§è¿­ä»£æ¬¡æ•°

# GPUé…ç½®
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"
```

### æ¨¡å‹é…ç½®

æ”¯æŒçš„æ¨¡å‹ç±»å‹ï¼š
- **GPTç³»åˆ—**: gpt-3.5-turbo, gpt-4, gpt-4o
- **FLAN-T5ç³»åˆ—**: flan-t5-base, flan-t5-large, flan-t5-xl, flan-t5-xxl
- **BERTç³»åˆ—**: bert-base-uncased (ç”¨äºè·³æ•°é¢„æµ‹)

## ğŸ“ˆ å®éªŒè¿è¡Œ

### è¿è¡Œå®Œæ•´å®éªŒ

```bash
cd ircot-main
./reproduce.sh ircot_qa gpt-4 hotpotqa
```

### æ‰¹é‡è¯„ä¼°

```bash
# è¯„ä¼°æ‰€æœ‰æ¨¡å‹åœ¨HotpotQAä¸Šçš„è¡¨ç°
python runner.py ircot_qa gpt-4 hotpotqa predict --prompt_set 1
python runner.py ircot_qa gpt-4 hotpotqa summarize --prompt_set aggregate --best --eval_test --official
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

ç³»ç»Ÿæ”¯æŒå¤šç§è¯„ä¼°æŒ‡æ ‡ï¼š

- **Exact Match (EM)**: ç²¾ç¡®åŒ¹é…
- **F1 Score**: F1åˆ†æ•°
- **EMCover**: ç­”æ¡ˆåŒ…å«åº¦
- **Recall**: å¬å›ç‡

### ç»“æœç¤ºä¾‹

```
[FINAL EVALUATION]
æ€»é—®é¢˜æ•°: 500
Exact Match (EM): 0.6247
F1 Score: 0.7156
Recall: 0.7892
EMCover: 0.7234
```

## ğŸ”§ APIæ¥å£

### RESTful API

å¯åŠ¨APIæœåŠ¡åï¼Œå¯ä»¥é€šè¿‡HTTPè¯·æ±‚è¿›è¡Œé—®ç­”ï¼š

```python
import requests

response = requests.post("http://localhost:8000/qa", json={
    "question": "Who lived longer, Muhammad Ali or Alan Turing?",
    "context": [...]
})

print(response.json()["answer"])
```

### Pythonæ¥å£

```python
from llm_client import LLMClient
from mcts_all import mcts_search

# åˆå§‹åŒ–å®¢æˆ·ç«¯
llm_client = LLMClient(api_key="your_key", base_url="your_url")

# æ‰§è¡ŒMCTSæœç´¢
answer = mcts_search(
    root=root_node,
    max_hop=3,
    max_iter=15,
    llm_client=llm_client,
    question=question,
    context=context
)
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### GPUåŠ é€Ÿ

```python
# å¤šGPUå¹¶è¡Œ
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

### ç¼“å­˜ä¼˜åŒ–

```python
# BGEåµŒå…¥ç¼“å­˜
bgeembedder = BGEEmbedder(
    model_name="bge_m3",
    embeddings_dir="embeddings_parts"
)
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æº - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…

## ğŸ™ è‡´è°¢

- [IRCoT](https://github.com/StonyBrookNLP/ircot) - é“¾å¼æ€ç»´æ£€ç´¢æ¡†æ¶
- [CommaQA](https://github.com/allenai/CommaQA) - å¤æ‚é—®ç­”ç³»ç»Ÿæ¡†æ¶
- [BGE](https://github.com/FlagOpen/FlagEmbedding) - é«˜è´¨é‡åµŒå…¥æ¨¡å‹
- [HotpotQA](https://hotpotqa.github.io/) - å¤šè·³é—®ç­”æ•°æ®é›†

## ğŸ“š ç›¸å…³è®ºæ–‡

```bibtex
@inproceedings{trivedi-etal-2023-interleaving,
    title = "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
    author = "Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish",
    booktitle = "Proceedings of ACL 2023",
    year = "2023"
}
```

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- å¼€å¯ [Issue](../../issues)
- é‚®ä»¶è”ç³»ï¼š[your-email@example.com]
- é¡¹ç›®ä¸»é¡µï¼š[é¡¹ç›®é“¾æ¥] 